{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhangKagTa19/News-Classification/blob/main/Softmax_TextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-Na14pvWAf3"
      },
      "source": [
        "# Logistic Regression with a Neural Network mindset\n",
        "\n",
        "Welcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize Politics-Society News. This assignment will step you through how to do this with a Neural Network mindset, and so will also hone your intuitions about deep learning.\n",
        "\n",
        "\n",
        "\n",
        "**You will learn to:**\n",
        "- Build the general architecture of a learning algorithm, including:\n",
        "    - Initializing parameters\n",
        "    - Calculating the cost function and its gradient\n",
        "    - Using an optimization algorithm (gradient descent)\n",
        "- Gather all three functions above into a main model function, in the right order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa02Fmu9WAf5"
      },
      "source": [
        "## 1 - Packages ##\n",
        "\n",
        "First, let's run the cell below to import all the packages that you will need during this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wF9a-jbjWAf5"
      },
      "outputs": [],
      "source": [
        "# libraries for dataset preparation, feature engineering\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import model_selection, preprocessing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "np.random.seed(123) #for reprodicible results\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMtW_qw9eDU0",
        "outputId": "96fd68d6-be99-41e4-d831-f9f5a06cc289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "SHpvjfBSWAf6"
      },
      "source": [
        "## 2 - Dataset preparation ##\n",
        "\n",
        "**Problem Statement**:\n",
        "\n",
        "You are given a dataset (\"news_dataset.csv\") containing a set of Vietnamses news labeled as politics-society (y=\"Chinhtrixahoi\"), sport (y=\"Thethao) or others (y=\"Khac\")\n",
        "* Actually, \"Khac\" comprises of \"Phap luat\", \"Kinh doanh\", \"Doi song\", \"The gioi\" news\n",
        "\n",
        "You will build a simple news-classification algorithm that can correctly classify news.\n",
        "\n",
        "Let's get more familiar with the dataset. Load the data by running the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwWKw1BYWAf6"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/NLP_LogisticClassification/Dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "kx-fnoFmWAf6",
        "outputId": "b34d0473-f944-4ce1-9b02-0f600c35dc8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text          label\n",
              "0  Trong khi đó, Indonesia với chiến thắng trước ...        Thethao\n",
              "1  Như vậy, tổng cộng Việt Nam đã bị trừ 41,06 đi...        Thethao\n",
              "2  Đến giờ, người hâm mộ bóng đá Việt Nam vẫn chư...        Thethao\n",
              "3  Không có gì chán hơn là làm việc và sống giữa ...           Khac\n",
              "4   Cún con chứa ma túy Cún con - nạn nhân của bọ...           Khac\n",
              "5   Xây dựng tổ chức Đoàn thật sự là người bạn củ...  Chinhtrixahoi\n",
              "6   Lãnh đạo Công ty Vissan: Đây là bài học đau x...           Khac\n",
              "7  Giải bóng rổ vô địch Hà Nội 2023 diễn ra từ ng...        Thethao\n",
              "8  Nội dung nữ, 5 đội bóng sẽ thi đấu vòng tròn t...        Thethao\n",
              "9  Ban tổ chức cho phép mỗi đội được đăng ký khôn...        Thethao"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1050c9c2-7a8c-406a-a45f-85dfe77c46b6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Trong khi đó, Indonesia với chiến thắng trước ...</td>\n",
              "      <td>Thethao</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Như vậy, tổng cộng Việt Nam đã bị trừ 41,06 đi...</td>\n",
              "      <td>Thethao</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Đến giờ, người hâm mộ bóng đá Việt Nam vẫn chư...</td>\n",
              "      <td>Thethao</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Không có gì chán hơn là làm việc và sống giữa ...</td>\n",
              "      <td>Khac</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Cún con chứa ma túy Cún con - nạn nhân của bọ...</td>\n",
              "      <td>Khac</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Xây dựng tổ chức Đoàn thật sự là người bạn củ...</td>\n",
              "      <td>Chinhtrixahoi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Lãnh đạo Công ty Vissan: Đây là bài học đau x...</td>\n",
              "      <td>Khac</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Giải bóng rổ vô địch Hà Nội 2023 diễn ra từ ng...</td>\n",
              "      <td>Thethao</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Nội dung nữ, 5 đội bóng sẽ thi đấu vòng tròn t...</td>\n",
              "      <td>Thethao</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Ban tổ chức cho phép mỗi đội được đăng ký khôn...</td>\n",
              "      <td>Thethao</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1050c9c2-7a8c-406a-a45f-85dfe77c46b6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1050c9c2-7a8c-406a-a45f-85dfe77c46b6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1050c9c2-7a8c-406a-a45f-85dfe77c46b6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-52124b60-d521-4de1-bc0a-be32bdfd9735\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-52124b60-d521-4de1-bc0a-be32bdfd9735')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-52124b60-d521-4de1-bc0a-be32bdfd9735 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# The dataframe has two columns, text and label\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "AYsuBQ3rWAf7",
        "outputId": "2ad410f1-d636-4774-c54d-ba1bc8edca6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of politics-society documents: 198\n",
            "Number of sport documents: 200\n",
            "Number of other documents: 202\n"
          ]
        }
      ],
      "source": [
        "size_chinhtri = df[df['label'] == 'Chinhtrixahoi'].shape[0]\n",
        "size_thethao = df[df['label'] == 'Thethao'].shape[0]\n",
        "size_others = df[df['label'] == 'Khac'].shape[0]\n",
        "print('Number of politics-society documents: %s' %size_chinhtri)\n",
        "print('Number of sport documents: %s' %size_thethao)\n",
        "print('Number of other documents: %s' %size_others)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Gdpvbmc1WAf7",
        "outputId": "2df97337-a36f-4688-e600-0e216c8a269d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Xây dựng tổ chức Đoàn thật sự là người bạn của thanh niên  \\n (NLĐ)- Sáng 26-3, Thành đoàn TPHCM đã tổ chức trọng thể lễ mít tinh kỷ niệm 75 năm ngày thành lập Đoàn TNCS Hồ Chí Minh (26.3.1931 - 26.3.2006).  \\n Các ông Nguyễn Minh Triết, Ủy viên Bộ Chính trị - Bí thư Thành ủy TP; Võ Văn Kiệt, nguyên Thủ tướng Chính phủ cùng lãnh đạo HĐND, UBND, các ban - ngành TP và 10.000 đoàn viên TNCS đã tham dự mít tinh.  \\n Ông Tất Thành Cang, Bí thư Thành đoàn TPHCM, ôn lại truyền thống vẻ vang của Đoàn TNCS Hồ Chí Minh và nhấn mạnh: “Tổ chức Đoàn đang đối mặt với những khó khăn, thách thức mới. Đó là tình trạng thất nghiệp, tệ nạn xã hội; một bộ phận thanh niên thiếu rèn luyện, chậm tiến, chưa ý thức được trách nhiệm của mình đối với bản thân, gia đình và xã hội... Do vậy, mỗi cán bộ, đoàn viên hãy nỗ lực vươn lên, sống xứng đáng với danh hiệu cao quý “Đoàn viên TNCS Hồ Chí Minh”.  \\n Thay mặt lãnh đạo TPHCM, bà Phạm Phương Thảo, Phó Bí thư Thành ủy, đã giao cho Thành đoàn TPHCM nhiệm vụ xây dựng hình ảnh người thanh niên trong thời kỳ công nghiệp hóa - hiện đại hóa; xây dựng tổ chức Đoàn thật sự là người bạn của thanh niên và chăm lo xây dựng đội ngũ cán bộ Đoàn đủ sức làm nòng cốt xây dựng Đoàn vững mạnh. ? Trước đó, tối 25-3, tại Hà Nội, Trung ương Đoàn TNCS Hồ Chí Minh đã long trọng tổ chức mít tinh kỷ niệm 75 năm thành lập Đoàn. Thay mặt Ban Chấp hành Trung ương Đảng, Tổng Bí thư Nông Đức Mạnh đã trao tặng Trung ương Đoàn và tuổi trẻ Việt Nam bức trướng mang dòng chữ “Phát huy truyền thống 75 năm xây dựng và phát triển, Đoàn TNCS Hồ Chí Minh và tuổi trẻ Việt Nam đoàn kết, xung kích, sáng tạo trong sự nghiệp xây dựng và bảo vệ Tổ quốc”.  \\n  \\n '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# an example of a politics-society news\n",
        "df['text'][5]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# an example of a sport news\n",
        "df['text'][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kgQvXXNyfdtA",
        "outputId": "1f844d13-82ca-4b93-963d-e40a304de6d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Đến giờ, người hâm mộ bóng đá Việt Nam vẫn chưa hiểu tại sao trung vệ Thanh Bình lại nắm và kéo áo cầu thủ Indonesia một cách lộ liễu đến vậy. Nhất là khi đó là một tình huống trong vòng 16m50.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBpUZ0T5WAf7"
      },
      "source": [
        "Next, we will split the dataset into training and test sets so that we can train and test classifier. Also, we will encode our target column so that it can be used in our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Jir3dGpMWAf7",
        "outputId": "6ba0f3cf-5439-41f6-a563-1d987e476992",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Chinhtrixahoi\" \"Thethao\" \"Khac\"] labels corresponds to [0 2 1] labels\n"
          ]
        }
      ],
      "source": [
        "# split the dataset into training and test datasets\n",
        "train_x, test_x, train_y, test_y = model_selection.train_test_split(df['text'], df['label'])\n",
        "\n",
        "# label encode the target variable, encode 3 labels\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "test_y = encoder.fit_transform(test_y)\n",
        "print('[\"Chinhtrixahoi\" \"Thethao\" \"Khac\"] labels corresponds to %s labels' %encoder.transform([\"Chinhtrixahoi\", \"Thethao\",\"Khac\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPcgu5VxWAf7"
      },
      "source": [
        "The next step is the feature engineering step. In this step, raw text data will be transformed into feature vectors and new features will be created using the existing dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVsi67dQWAf7"
      },
      "source": [
        "### TF-IDF Vectors as features\n",
        "TF-IDF score represents the relative importance of a term in the document and the entire corpus. TF-IDF score is composed by two terms: the first computes the normalized Term Frequency (TF), the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
        "\n",
        "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
        "\n",
        "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
        "\n",
        "TF-IDF Vectors can be generated at different levels of input tokens (words, characters, n-grams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_4NfEqmWAf7"
      },
      "outputs": [],
      "source": [
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "tfidf_vect.fit(df['text'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xtest_tfidf =  tfidf_vect.transform(test_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OodqenpjWAf7",
        "outputId": "b8182efb-6320-4191-8774-f8ec4bb3827d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training documents: 450\n",
            "Number of testing documents: 150\n",
            "Number of features of each document: 5000\n",
            "xtrain_tfidf shape: (450, 5000)\n",
            "train_y shape: (450,)\n",
            "xtest_tfidf shape: (150, 5000)\n",
            "test_y shape: (150,)\n"
          ]
        }
      ],
      "source": [
        "# Getting transformed training and testing dataset\n",
        "print('Number of training documents: %s' %str(xtrain_tfidf.shape[0]))\n",
        "print('Number of testing documents: %s' %str(xtest_tfidf.shape[0]))\n",
        "print('Number of features of each document: %s' %str(xtrain_tfidf.shape[1]))\n",
        "print('xtrain_tfidf shape: %s' %str(xtrain_tfidf.shape))\n",
        "print('train_y shape: %s' %str(train_y.shape))\n",
        "print('xtest_tfidf shape: %s' %str(xtest_tfidf.shape))\n",
        "print('test_y shape: %s' %str(test_y.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFK45iKFWAf8"
      },
      "source": [
        "Many software bugs in deep learning come from having matrix/vector dimensions that don't fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs.\n",
        "\n",
        "**Exercise:**: For convenience, you should now transpose the training and testing numpy-array, and expand the shape of the lable arrays in the axis=0 position.\n",
        "\n",
        "After this, our training (and test) dataset is a numpy-array where each column represents a document vector. There should be the number of training documents (respectively the number of testing documents) as the number of columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lwk8NEgGWAf8"
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ###\n",
        "train_y = np.expand_dims(train_y, axis=0)\n",
        "test_y = np.expand_dims(test_y, axis=0)\n",
        "\n",
        "# for convenience in this exercise, we also use toarray() to convert sparse to dense matrix\n",
        "xtrain_tfidf =  xtrain_tfidf.T.toarray()\n",
        "xtest_tfidf =  xtest_tfidf.T.toarray()\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K32vgff4WAf8",
        "outputId": "1f4c4c77-4d31-4734-ecee-de1c85d5f57b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xtrain_tfidf shape: (5000, 450)\n",
            "train_y shape: (1, 450)\n",
            "xtest_tfidf shape: (5000, 150)\n",
            "test_y shape: (1, 150)\n"
          ]
        }
      ],
      "source": [
        "# New shape\n",
        "print('xtrain_tfidf shape: %s' %str(xtrain_tfidf.shape))\n",
        "print('train_y shape: %s' %str(train_y.shape))\n",
        "print('xtest_tfidf shape: %s' %str(xtest_tfidf.shape))\n",
        "print('test_y shape: %s' %str(test_y.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_9UqNbJWAf8"
      },
      "source": [
        "## 3 - General Architecture of the learning algorithm ##\n",
        "\n",
        "It's time to design a simple algorithm to distinguish politics-society news from other news.\n",
        "\n",
        "You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why **Logistic Regression is actually a very simple Neural Network!**\n",
        "\n",
        "**The below image is an analogy of the network architecture of an image classifier to a text classifier.**\n",
        "\n",
        "<img src=\"images/LogReg_kiank.png\" style=\"width:650px;height:400px;\">\n",
        "\n",
        "**Mathematical expression of the algorithm**:\n",
        "\n",
        "For one example $x^{(i)}$:\n",
        "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
        "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$\n",
        "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
        "\n",
        "The cost is then computed by summing over all training examples:\n",
        "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n",
        "\n",
        "**Key steps**:\n",
        "In this exercise, you will carry out the following steps:\n",
        "    - Initialize the parameters of the model\n",
        "    - Learn the parameters for the model by minimizing the cost  \n",
        "    - Use the learned parameters to make predictions (on the test set)\n",
        "    - Analyse the results and conclude"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI6fU2JsWAf8"
      },
      "source": [
        "## 4 - Building the parts of our algorithm ##\n",
        "\n",
        "The main steps for building a Neural Network are:\n",
        "1. Define the model structure (such as number of input features)\n",
        "2. Initialize the model's parameters\n",
        "3. Loop:\n",
        "    - Calculate current loss (forward propagation)\n",
        "    - Calculate current gradient (backward propagation)\n",
        "    - Update parameters (gradient descent)\n",
        "\n",
        "You often build 1-3 separately and integrate them into one function we call `model()`.\n",
        "\n",
        "### 4.1 - Helper functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *softmax function*, invented in 1959 by the social scientist\n",
        "R Duncan Luce in the context of *choice models* does precisely this.\n",
        "To transform our logits such that they become nonnegative and sum to $1$,\n",
        "while requiring that the model remains differentiable,\n",
        "we first exponentiate each logit (ensuring non-negativity)\n",
        "and then divide by their sum (ensuring that they sum to $1$).\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o})\\quad \\text{where}\\quad\n",
        "\\hat{y}_i = \\frac{\\exp(o_i)}{\\sum_j \\exp(o_j)}.\n",
        "$$"
      ],
      "metadata": {
        "id": "Aj-VwgbqLDto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(Z):\n",
        "    e_Z = np.exp(Z - np.max(Z, axis = 0, keepdims = True))\n",
        "    A = e_Z / e_Z.sum(axis = 0)\n",
        "    return A\n",
        "\n"
      ],
      "metadata": {
        "id": "zxp3aLlGL7iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#test function\n",
        "print (\"softmax([0 1 0 0 1 2]) = \" + str(softmax(np.array([0,1,0,0,1,2]))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cM67pDHSME0W",
        "outputId": "48919ed3-8af5-49dc-ef4a-b4e25f08b993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax([0 1 0 0 1 2]) = [0.06318868 0.17176464 0.06318868 0.06318868 0.17176464 0.46690469]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse\n",
        "def convert_labels(y, C = 3):\n",
        "    \"\"\"\n",
        "    convert 1d label to a matrix label: each column of this\n",
        "    matrix coresponding to 1 element in y. In i-th column of Y,\n",
        "    only one non-zeros element located in the y[i]-th position,\n",
        "    and = 1 ex: y = [0, 2, 1, 0], and 3 classes then return\n",
        "\n",
        "            [[1, 0, 0, 1],\n",
        "             [0, 0, 1, 0],\n",
        "             [0, 1, 0, 0]]\n",
        "    \"\"\"\n",
        "    Y = sparse.coo_matrix((np.ones_like(y),\n",
        "        (y, np.arange(len(y)))), shape = (C, len(y))).toarray()\n",
        "    return Y\n"
      ],
      "metadata": {
        "id": "WBX9BRoxosPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label1 = convert_labels([1,2,0], )\n",
        "print(label1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5_EBtvjo0GA",
        "outputId": "c066cb1d-245f-4a5c-e321-07913c164443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1]\n",
            " [1 0 0]\n",
            " [0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcq5zRpxWAf9"
      },
      "source": [
        "### 4.2 - Initializing parameters\n",
        "\n",
        "**Exercise:** Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don't know what numpy function to use, look up np.zeros() in the Numpy library's documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4zacY2jWAf-"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: initialize_with_zeros\n",
        "\n",
        "def initialize_with_zeros(dim):\n",
        "    \"\"\"\n",
        "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
        "\n",
        "    Argument:\n",
        "    dim -- size of the w vector we want (or number of parameters in this case)\n",
        "\n",
        "    Returns:\n",
        "    w -- initialized vector of shape (dim, 1)\n",
        "    b -- initialized scalar (corresponds to the bias)\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    w = np.zeros((dim, 3)) #3 is the num of label\n",
        "    b = 0.\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    assert(w.shape == (dim, 3))\n",
        "    assert(isinstance(b, float) or isinstance(b, int))\n",
        "\n",
        "    return w, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If_KPgyVWAf-",
        "outputId": "c8f3fd84-5a19-4445-aef7-8331ea3e8cde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = [[0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "b = 0.0\n"
          ]
        }
      ],
      "source": [
        "dim = 2\n",
        "w, b = initialize_with_zeros(dim)\n",
        "print (\"w = \" + str(w))\n",
        "print (\"b = \" + str(b))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5G3ifkUWAf-"
      },
      "source": [
        "### Softmax and Derivatives\n",
        "\n",
        "Since the softmax and the corresponding loss are so common,\n",
        "it is worth while understanding a bit better how it is computed.\n",
        "Plugging $o$ into the definition of the loss $l$\n",
        "and using the definition of the softmax we obtain:\n",
        "\n",
        "$$\n",
        "l = -\\sum_j y_j \\log \\hat{y}_j = \\sum_j y_j \\log \\sum_k \\exp(o_k) - \\sum_j y_j o_j\n",
        "= \\log \\sum_k \\exp(o_k) - \\sum_j y_j o_j.\n",
        "$$\n",
        "\n",
        "To understand a bit better what is going on,\n",
        "consider the derivative with respect to $o$. We get\n",
        "\n",
        "$$\n",
        "\\partial_{o_j} l = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)} - y_j = \\mathrm{softmax}(\\mathbf{o})_j - y_j = P(y = j \\mid x) - y_j.\n",
        "$$\n",
        "\n",
        "In other words, the gradient is the difference\n",
        "between the probability assigned to the true class by our model,\n",
        "as expressed by the probability $P(y \\mid x)$,\n",
        "and what actually happened, as expressed by $y$.\n",
        "In this sense, it is very similar to what we saw in regression,\n",
        "where the gradient was the difference\n",
        "between the observation $y$ and estimate $\\hat{y}$. This is not coincidence.\n",
        "In any [exponential family](https://en.wikipedia.org/wiki/Exponential_family) model,\n",
        "the gradients of the log-likelihood are given by precisely this term.\n",
        "This fact makes computing gradients easy in practice.\n",
        "### Cross-Entropy Loss\n",
        "\n",
        "Now consider the case where we observe not just a single outcome\n",
        "but an entire distribution over outcomes.\n",
        "We can use the same representation as before for $y$.\n",
        "The only difference is that rather than a vector containing only binary entries,\n",
        "say $(0, 0, 1)$, we now have a generic probability vector, say $(0.1, 0.2, 0.7)$.\n",
        "The math that we used previously to define the loss $l$ still works out fine,\n",
        "just that the interpretation is slightly more general.\n",
        "It is the expected value of the loss for a distribution over labels.\n",
        "\n",
        "$$\n",
        "l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_j y_j \\log \\hat{y}_j.\n",
        "$$\n",
        "\n",
        "This loss is called the cross-entropy loss and it is\n",
        "one of the most commonly used losses for multiclass classification.\n",
        "We can demystify the name by introducing the basics of information theory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPGxvLVoWAf-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# GRADED FUNCTION: propagate\n",
        "def compute_cost(A, Y):\n",
        "    return -np.sum(Y*np.log10(A))\n",
        "\n",
        "def grad(X, A, Y):\n",
        "    E = A - Y\n",
        "\n",
        "    return X.dot(E.T)\n",
        "\n",
        "def propagate(w, b, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function and its gradient for the propagation explained above\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector of size (1, number of examples)\n",
        "\n",
        "    Return:\n",
        "    cost -- negative log-likelihood cost for logistic regression\n",
        "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
        "    db -- gradient of the loss with respect to b, thus same shape as b\n",
        "    \"\"\"\n",
        "\n",
        "    one_hot_vector = convert_labels(Y, )\n",
        "\n",
        "\n",
        "    # FORWARD PROPAGATION (FROM X TO COST)\n",
        "    Z = np.dot(w.T, X) + b\n",
        "    A = softmax(Z)  # compute activation\n",
        "\n",
        "    # COST FUNCTION\n",
        "    cost = compute_cost(A, one_hot_vector)\n",
        "\n",
        "    # BACKWARD PROPAGATION (TO FIND GRADIENTS)\n",
        "\n",
        "    dw = grad(X, A, one_hot_vector)\n",
        "\n",
        "\n",
        "    # Reshape dw to match the shape of w\n",
        "    dw = dw.reshape(w.shape)\n",
        "\n",
        "    grads = {\"dw\": dw, \"db\": 0}\n",
        "\n",
        "    return grads, cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoQyP7LmWAf_",
        "outputId": "4a3077ef-e699-46d3-a08e-a0a95bbd7e75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost for X1: 0.24040963846679836\n"
          ]
        }
      ],
      "source": [
        "# Given data\n",
        "L = 3  # Number of classes\n",
        "X1 = np.array([[2],[1]])\n",
        "\n",
        "\n",
        "# Assuming you have weights w and bias b properly initialized\n",
        "# Initialize w and b with the correct shapes\n",
        "# Specify weights\n",
        "W1 = np.array([0.2, 0.7])\n",
        "W2 = np.array([0.8, 0.4])\n",
        "W3 = np.array([0.3, 0.3])\n",
        "b=0\n",
        "# Combine weights into a matrix W with the correct shape\n",
        "w = np.vstack([W1, W2, W3]).T  # Transpose to have the correct shape\n",
        "\n",
        "# w, b = initialize_with_zeros(X1.shape[0])\n",
        "# Reshape X1 to be a column vector\n",
        "\n",
        "\n",
        "\n",
        "# Convert Y1 to one-hot encoded matrix\n",
        "Y1=[1]\n",
        "# Forward propagation and compute cost using the propagate function\n",
        "grads1, cost1 = propagate(w, b, X1, Y1)\n",
        "\n",
        "print(\"Cost for X1:\", cost1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug9CWfUFWAf_"
      },
      "source": [
        "### 4.4 - Optimization\n",
        "- You have initialized your parameters.\n",
        "- You are also able to compute a cost function and its gradient.\n",
        "- Now, you want to update the parameters using gradient descent.\n",
        "\n",
        "**Exercise:** Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FnX5rF8WAf_"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: optimize\n",
        "\n",
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector, of shape (1, number of examples)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- True to print the loss every 100 steps\n",
        "\n",
        "    Returns:\n",
        "    params -- dictionary containing the weights w and bias b\n",
        "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
        "\n",
        "    Tips:\n",
        "    You basically need to write down two steps and iterate through them:\n",
        "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
        "        2) Update the parameters using gradient descent rule for w and b.\n",
        "    \"\"\"\n",
        "\n",
        "    costs = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "\n",
        "\n",
        "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
        "        ### START CODE HERE ###\n",
        "        grads, cost = propagate(w,b,X,Y)\n",
        "        ### END CODE HERE ###\n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "\n",
        "        # update rule (≈ 2 lines of code)\n",
        "        ### START CODE HERE ###\n",
        "        w = w - learning_rate*dw\n",
        "        b = b - learning_rate*db\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Record the costs\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "        # Print the cost every 100 training iterations\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "\n",
        "    return params, grads, costs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_5ql1PDWAf_",
        "outputId": "0d8f3241-1806-4c88-f4d8-815982c2060a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 1)\n",
            "[1]\n",
            "w = [[-0.02676149  1.22077943  0.10598206]\n",
            " [ 0.58661925  0.61038971  0.20299103]]\n",
            "b = 0.0\n",
            "dw = [[ 0.14095025 -0.26615014  0.12519988]\n",
            " [ 0.07047513 -0.13307507  0.06259994]]\n",
            "db = 0\n"
          ]
        }
      ],
      "source": [
        "print(X1.shape)\n",
        "print(Y1)\n",
        "\n",
        "params, grads, costs = optimize(w, b, X1, Y1, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
        "\n",
        "print (\"w = \" + str(params[\"w\"]))\n",
        "print (\"b = \" + str(params[\"b\"]))\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0vpJNvGWAf_"
      },
      "source": [
        "**Exercise:** The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the `predict()` function. There are two steps to computing predictions:\n",
        "\n",
        "1. Calculate $\\hat{Y} = A = softmax(w^T X + b)$\n",
        "\n",
        "2. The index of the argmax of probability will be the label for that X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F3pW0GLWAgA"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: predict\n",
        "\n",
        "def predict(w, b, X):\n",
        "    Z = np.dot(w.T, X) + b\n",
        "    A = softmax(Z)  # compute activation\n",
        "    return np.argmax(A, axis = 0), A\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUpTVQkTWAgA",
        "outputId": "5fa5e897-9832-4da7-ca7c-4f5dc2b9dec3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Classes:\n",
            "[1]\n",
            "Softmax Output:\n",
            "[[0.06991584]\n",
            " [0.867959  ]\n",
            " [0.06212516]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Make a prediction\n",
        "predicted_classes, A = predict(params[\"w\"], params[\"b\"], X1)\n",
        "\n",
        "# Print the results\n",
        "print(\"Predicted Classes:\")\n",
        "print(predicted_classes)\n",
        "print(\"Softmax Output:\")\n",
        "print(A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CguO6AO6WAgA"
      },
      "source": [
        "**Expected Output**:\n",
        "\n",
        "<table style=\"width:30%\">\n",
        "    <tr>\n",
        "         <td>\n",
        "             **predictions**\n",
        "         </td>\n",
        "          <td>\n",
        "            1\n",
        "         </td>  \n",
        "   </tr>\n",
        "\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In9AKy0-WAgI"
      },
      "source": [
        "## 5 - Merge all functions into a model ##\n",
        "\n",
        "You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n",
        "\n",
        "**Exercise:** Implement the model function. Use the following notation:\n",
        "    - Y_prediction_test for your predictions on the test set\n",
        "    - Y_prediction_train for your predictions on the train set\n",
        "    - w, costs, grads for the outputs of optimize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACipixumWAgI"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: model\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, tol=1e-4, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function you've implemented previously\n",
        "\n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
        "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
        "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
        "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_cost -- Set to true to print the cost every 100 iterations\n",
        "\n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    X = X_train\n",
        "    Y_train = Y_train\n",
        "    W, b = initialize_with_zeros(X_train.shape[0])  # Assuming initialize_with_zeros is implemented correctly\n",
        "    C = W.shape[1]\n",
        "    N = X.shape[1]\n",
        "    d = X.shape[1]\n",
        "\n",
        "    costs = []  # Initialize costs list\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    count = 0\n",
        "    check_w_after = 20\n",
        "    while count < num_iterations:\n",
        "        # mix data\n",
        "        mix_id = np.random.permutation(N)\n",
        "        for i in mix_id:\n",
        "            xi = X[:, i].reshape(-1,1)\n",
        "            yi = Y_train[:, i]\n",
        "\n",
        "            grads, cost = propagate(W, b, xi, yi)\n",
        "\n",
        "            dw = grads[\"dw\"]\n",
        "            db = grads[\"db\"]\n",
        "            W = W - learning_rate * dw\n",
        "            b = b - learning_rate * db\n",
        "            if i % 100 == 0:\n",
        "                costs.append(cost)\n",
        "            if print_cost and i % 100 == 0:\n",
        "                print(\"Cost after iteration %i: %f\" % (i, cost))\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    # Predict test/train set examples\n",
        "    Y_prediction_test = predict(W, b, X_test)[0]\n",
        "    Y_prediction_train = predict(W, b, X_train)[0]\n",
        "\n",
        "    # Print train/test Errors\n",
        "    train_accuracy = np.mean(Y_prediction_train == Y_train) * 100\n",
        "    test_accuracy = np.mean(Y_prediction_test == Y_test) * 100\n",
        "    print(\"train accuracy: {} %\".format(train_accuracy))\n",
        "    print(\"test accuracy: {} %\".format(test_accuracy))\n",
        "\n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test,\n",
        "         \"Y_prediction_train\": Y_prediction_train,\n",
        "         \"w\": W,\n",
        "         \"b\": b,\n",
        "         \"learning_rate\": learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "\n",
        "    return d\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7nzlMhSWAgI"
      },
      "source": [
        "Run the following cell to train your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYor-0eRWAgI",
        "outputId": "57d7016f-798b-40e5-bdec-c2bd2ebfc570",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 200: 0.420942\n",
            "Cost after iteration 300: 0.194470\n",
            "Cost after iteration 400: 0.347466\n",
            "Cost after iteration 100: 0.260340\n",
            "Cost after iteration 0: 0.534059\n",
            "Cost after iteration 200: 0.211118\n",
            "Cost after iteration 400: 0.292377\n",
            "Cost after iteration 100: 0.161311\n",
            "Cost after iteration 0: 0.343676\n",
            "Cost after iteration 300: 0.144117\n",
            "Cost after iteration 0: 0.259876\n",
            "Cost after iteration 200: 0.099516\n",
            "Cost after iteration 400: 0.180868\n",
            "Cost after iteration 100: 0.100896\n",
            "Cost after iteration 300: 0.119157\n",
            "Cost after iteration 0: 0.162692\n",
            "Cost after iteration 100: 0.058674\n",
            "Cost after iteration 400: 0.122887\n",
            "Cost after iteration 200: 0.068596\n",
            "Cost after iteration 300: 0.070171\n",
            "Cost after iteration 300: 0.072112\n",
            "Cost after iteration 0: 0.143464\n",
            "Cost after iteration 200: 0.057366\n",
            "Cost after iteration 100: 0.048534\n",
            "Cost after iteration 400: 0.115418\n",
            "Cost after iteration 0: 0.107646\n",
            "Cost after iteration 100: 0.037013\n",
            "Cost after iteration 200: 0.042540\n",
            "Cost after iteration 400: 0.106263\n",
            "Cost after iteration 300: 0.055998\n",
            "Cost after iteration 200: 0.040165\n",
            "Cost after iteration 300: 0.046571\n",
            "Cost after iteration 100: 0.036937\n",
            "Cost after iteration 0: 0.086968\n",
            "Cost after iteration 400: 0.083097\n",
            "Cost after iteration 0: 0.074323\n",
            "Cost after iteration 300: 0.045685\n",
            "Cost after iteration 400: 0.077608\n",
            "Cost after iteration 100: 0.024759\n",
            "Cost after iteration 200: 0.032673\n",
            "Cost after iteration 0: 0.066431\n",
            "Cost after iteration 200: 0.029954\n",
            "Cost after iteration 400: 0.070843\n",
            "Cost after iteration 100: 0.020982\n",
            "Cost after iteration 300: 0.038065\n",
            "Cost after iteration 100: 0.020892\n",
            "Cost after iteration 0: 0.059752\n",
            "Cost after iteration 400: 0.057054\n",
            "Cost after iteration 200: 0.026154\n",
            "Cost after iteration 300: 0.035685\n",
            "Cost after iteration 100: 0.018715\n",
            "Cost after iteration 400: 0.056525\n",
            "Cost after iteration 200: 0.023942\n",
            "Cost after iteration 300: 0.034261\n",
            "Cost after iteration 0: 0.053188\n",
            "Cost after iteration 400: 0.054145\n",
            "Cost after iteration 0: 0.049523\n",
            "Cost after iteration 300: 0.032431\n",
            "Cost after iteration 200: 0.022425\n",
            "Cost after iteration 100: 0.015604\n",
            "Cost after iteration 0: 0.043443\n",
            "Cost after iteration 100: 0.014992\n",
            "Cost after iteration 300: 0.028421\n",
            "Cost after iteration 200: 0.020440\n",
            "Cost after iteration 400: 0.047598\n",
            "Cost after iteration 400: 0.041462\n",
            "Cost after iteration 300: 0.028752\n",
            "Cost after iteration 0: 0.039921\n",
            "Cost after iteration 200: 0.019274\n",
            "Cost after iteration 100: 0.013172\n",
            "Cost after iteration 400: 0.040180\n",
            "Cost after iteration 100: 0.011855\n",
            "Cost after iteration 0: 0.037808\n",
            "Cost after iteration 200: 0.018078\n",
            "Cost after iteration 300: 0.026605\n",
            "Cost after iteration 0: 0.035371\n",
            "Cost after iteration 200: 0.017356\n",
            "Cost after iteration 400: 0.039223\n",
            "Cost after iteration 100: 0.012460\n",
            "Cost after iteration 300: 0.024259\n",
            "Cost after iteration 0: 0.033156\n",
            "Cost after iteration 400: 0.036226\n",
            "Cost after iteration 100: 0.010523\n",
            "Cost after iteration 200: 0.015960\n",
            "Cost after iteration 300: 0.021263\n",
            "Cost after iteration 100: 0.009618\n",
            "Cost after iteration 0: 0.031066\n",
            "Cost after iteration 400: 0.033654\n",
            "Cost after iteration 200: 0.015192\n",
            "Cost after iteration 300: 0.020755\n",
            "Cost after iteration 100: 0.010088\n",
            "Cost after iteration 300: 0.020693\n",
            "Cost after iteration 0: 0.028885\n",
            "Cost after iteration 400: 0.031309\n",
            "Cost after iteration 200: 0.014420\n",
            "Cost after iteration 400: 0.031721\n",
            "Cost after iteration 0: 0.027516\n",
            "Cost after iteration 100: 0.008280\n",
            "Cost after iteration 300: 0.017700\n",
            "Cost after iteration 200: 0.013643\n",
            "Cost after iteration 0: 0.026040\n",
            "Cost after iteration 400: 0.028888\n",
            "Cost after iteration 300: 0.017129\n",
            "Cost after iteration 200: 0.012992\n",
            "Cost after iteration 100: 0.007970\n",
            "Cost after iteration 400: 0.029059\n",
            "Cost after iteration 100: 0.007850\n",
            "Cost after iteration 300: 0.016722\n",
            "Cost after iteration 200: 0.012601\n",
            "Cost after iteration 0: 0.024610\n",
            "Cost after iteration 0: 0.023466\n",
            "Cost after iteration 100: 0.007560\n",
            "Cost after iteration 200: 0.012357\n",
            "Cost after iteration 300: 0.016849\n",
            "Cost after iteration 400: 0.027368\n",
            "Cost after iteration 100: 0.007012\n",
            "Cost after iteration 200: 0.011524\n",
            "Cost after iteration 400: 0.026124\n",
            "Cost after iteration 0: 0.022043\n",
            "Cost after iteration 300: 0.015304\n",
            "Cost after iteration 300: 0.014982\n",
            "Cost after iteration 0: 0.021683\n",
            "Cost after iteration 100: 0.006718\n",
            "Cost after iteration 200: 0.011174\n",
            "Cost after iteration 400: 0.025021\n",
            "Cost after iteration 200: 0.010690\n",
            "Cost after iteration 400: 0.024185\n",
            "Cost after iteration 100: 0.006286\n",
            "Cost after iteration 0: 0.020429\n",
            "Cost after iteration 300: 0.014590\n",
            "Cost after iteration 400: 0.023304\n",
            "Cost after iteration 100: 0.005897\n",
            "Cost after iteration 200: 0.010440\n",
            "Cost after iteration 300: 0.013561\n",
            "Cost after iteration 0: 0.019599\n",
            "Cost after iteration 400: 0.022500\n",
            "Cost after iteration 100: 0.005942\n",
            "Cost after iteration 0: 0.019063\n",
            "Cost after iteration 200: 0.010079\n",
            "Cost after iteration 300: 0.014020\n",
            "Cost after iteration 200: 0.009839\n",
            "Cost after iteration 300: 0.013285\n",
            "Cost after iteration 100: 0.005592\n",
            "Cost after iteration 400: 0.021574\n",
            "Cost after iteration 0: 0.018274\n",
            "Cost after iteration 100: 0.005499\n",
            "Cost after iteration 400: 0.021287\n",
            "Cost after iteration 0: 0.017374\n",
            "Cost after iteration 300: 0.012611\n",
            "Cost after iteration 200: 0.009443\n",
            "Cost after iteration 100: 0.005282\n",
            "Cost after iteration 400: 0.020972\n",
            "Cost after iteration 300: 0.011838\n",
            "Cost after iteration 200: 0.009101\n",
            "Cost after iteration 0: 0.017026\n",
            "Cost after iteration 400: 0.019865\n",
            "Cost after iteration 0: 0.016790\n",
            "Cost after iteration 200: 0.008970\n",
            "Cost after iteration 100: 0.005202\n",
            "Cost after iteration 300: 0.011716\n",
            "Cost after iteration 0: 0.015997\n",
            "Cost after iteration 300: 0.011715\n",
            "Cost after iteration 100: 0.004934\n",
            "Cost after iteration 400: 0.019569\n",
            "Cost after iteration 200: 0.008579\n",
            "Cost after iteration 100: 0.004471\n",
            "Cost after iteration 400: 0.017876\n",
            "Cost after iteration 300: 0.010883\n",
            "Cost after iteration 200: 0.008367\n",
            "Cost after iteration 0: 0.015531\n",
            "Cost after iteration 400: 0.018050\n",
            "Cost after iteration 200: 0.008002\n",
            "Cost after iteration 300: 0.010728\n",
            "Cost after iteration 0: 0.015094\n",
            "Cost after iteration 100: 0.004462\n",
            "Cost after iteration 200: 0.007976\n",
            "Cost after iteration 0: 0.014637\n",
            "Cost after iteration 300: 0.010591\n",
            "Cost after iteration 400: 0.017467\n",
            "Cost after iteration 100: 0.004237\n",
            "Cost after iteration 200: 0.007676\n",
            "Cost after iteration 0: 0.014016\n",
            "Cost after iteration 400: 0.016960\n",
            "Cost after iteration 300: 0.010344\n",
            "Cost after iteration 100: 0.004159\n",
            "Cost after iteration 400: 0.016349\n",
            "Cost after iteration 200: 0.007532\n",
            "Cost after iteration 0: 0.013856\n",
            "Cost after iteration 100: 0.003991\n",
            "Cost after iteration 300: 0.009955\n",
            "Cost after iteration 200: 0.007373\n",
            "Cost after iteration 400: 0.016129\n",
            "Cost after iteration 300: 0.009752\n",
            "Cost after iteration 0: 0.013306\n",
            "Cost after iteration 100: 0.003946\n",
            "Cost after iteration 100: 0.003820\n",
            "Cost after iteration 200: 0.007133\n",
            "Cost after iteration 300: 0.009574\n",
            "Cost after iteration 0: 0.012855\n",
            "Cost after iteration 400: 0.015915\n",
            "Cost after iteration 400: 0.015324\n",
            "Cost after iteration 0: 0.012646\n",
            "Cost after iteration 100: 0.003826\n",
            "Cost after iteration 200: 0.006980\n",
            "Cost after iteration 300: 0.009315\n",
            "Cost after iteration 100: 0.003682\n",
            "Cost after iteration 400: 0.015043\n",
            "Cost after iteration 0: 0.012293\n",
            "Cost after iteration 300: 0.009180\n",
            "Cost after iteration 200: 0.006815\n",
            "Cost after iteration 200: 0.006696\n",
            "Cost after iteration 100: 0.003569\n",
            "Cost after iteration 300: 0.009176\n",
            "Cost after iteration 400: 0.014157\n",
            "Cost after iteration 0: 0.011974\n",
            "Cost after iteration 100: 0.003438\n",
            "Cost after iteration 200: 0.006548\n",
            "Cost after iteration 0: 0.011652\n",
            "Cost after iteration 400: 0.014251\n",
            "Cost after iteration 300: 0.008841\n",
            "Cost after iteration 200: 0.006474\n",
            "Cost after iteration 100: 0.003451\n",
            "Cost after iteration 300: 0.008665\n",
            "Cost after iteration 400: 0.013755\n",
            "Cost after iteration 0: 0.011500\n",
            "Cost after iteration 300: 0.008412\n",
            "Cost after iteration 100: 0.003379\n",
            "Cost after iteration 400: 0.014015\n",
            "Cost after iteration 200: 0.006307\n",
            "Cost after iteration 0: 0.011274\n",
            "Cost after iteration 0: 0.011029\n",
            "Cost after iteration 400: 0.013443\n",
            "Cost after iteration 100: 0.003094\n",
            "Cost after iteration 200: 0.006101\n",
            "Cost after iteration 300: 0.008203\n",
            "Cost after iteration 100: 0.003094\n",
            "Cost after iteration 200: 0.005967\n",
            "Cost after iteration 300: 0.008302\n",
            "Cost after iteration 400: 0.013297\n",
            "Cost after iteration 0: 0.010656\n",
            "Cost after iteration 300: 0.007898\n",
            "Cost after iteration 400: 0.012658\n",
            "Cost after iteration 200: 0.005861\n",
            "Cost after iteration 0: 0.010502\n",
            "Cost after iteration 100: 0.002997\n",
            "Cost after iteration 100: 0.002954\n",
            "Cost after iteration 400: 0.012293\n",
            "Cost after iteration 200: 0.005724\n",
            "Cost after iteration 0: 0.010284\n",
            "Cost after iteration 300: 0.007740\n",
            "Cost after iteration 100: 0.002890\n",
            "Cost after iteration 300: 0.007599\n",
            "Cost after iteration 400: 0.012302\n",
            "Cost after iteration 0: 0.010144\n",
            "Cost after iteration 200: 0.005666\n",
            "Cost after iteration 300: 0.007603\n",
            "Cost after iteration 400: 0.012026\n",
            "Cost after iteration 0: 0.009804\n",
            "Cost after iteration 200: 0.005566\n",
            "Cost after iteration 100: 0.002748\n",
            "Cost after iteration 200: 0.005464\n",
            "Cost after iteration 300: 0.007295\n",
            "Cost after iteration 400: 0.011712\n",
            "Cost after iteration 0: 0.009628\n",
            "Cost after iteration 100: 0.002786\n",
            "Cost after iteration 200: 0.005377\n",
            "Cost after iteration 100: 0.002750\n",
            "Cost after iteration 400: 0.011513\n",
            "Cost after iteration 0: 0.009483\n",
            "Cost after iteration 300: 0.007376\n",
            "Cost after iteration 100: 0.002658\n",
            "Cost after iteration 300: 0.007043\n",
            "Cost after iteration 200: 0.005303\n",
            "Cost after iteration 400: 0.011402\n",
            "Cost after iteration 0: 0.009305\n",
            "Cost after iteration 200: 0.005219\n",
            "Cost after iteration 100: 0.002626\n",
            "Cost after iteration 300: 0.006928\n",
            "Cost after iteration 0: 0.009177\n",
            "Cost after iteration 400: 0.011343\n",
            "Cost after iteration 300: 0.006909\n",
            "Cost after iteration 100: 0.002583\n",
            "Cost after iteration 0: 0.008966\n",
            "Cost after iteration 200: 0.005097\n",
            "Cost after iteration 400: 0.010921\n",
            "Cost after iteration 200: 0.004974\n",
            "Cost after iteration 300: 0.006789\n",
            "Cost after iteration 0: 0.008823\n",
            "Cost after iteration 100: 0.002479\n",
            "Cost after iteration 400: 0.010744\n",
            "Cost after iteration 200: 0.004947\n",
            "Cost after iteration 0: 0.008626\n",
            "Cost after iteration 300: 0.006600\n",
            "Cost after iteration 400: 0.010675\n",
            "Cost after iteration 100: 0.002443\n",
            "Cost after iteration 100: 0.002428\n",
            "Cost after iteration 300: 0.006638\n",
            "Cost after iteration 0: 0.008478\n",
            "Cost after iteration 200: 0.004832\n",
            "Cost after iteration 400: 0.010387\n",
            "Cost after iteration 200: 0.004778\n",
            "Cost after iteration 100: 0.002346\n",
            "Cost after iteration 0: 0.008355\n",
            "Cost after iteration 400: 0.010282\n",
            "Cost after iteration 300: 0.006433\n",
            "Cost after iteration 300: 0.006351\n",
            "Cost after iteration 400: 0.009927\n",
            "Cost after iteration 100: 0.002329\n",
            "Cost after iteration 200: 0.004734\n",
            "Cost after iteration 0: 0.008267\n",
            "Cost after iteration 400: 0.009914\n",
            "Cost after iteration 0: 0.008100\n",
            "Cost after iteration 300: 0.006266\n",
            "Cost after iteration 100: 0.002289\n",
            "Cost after iteration 200: 0.004619\n",
            "Cost after iteration 300: 0.006177\n",
            "Cost after iteration 100: 0.002247\n",
            "Cost after iteration 400: 0.009609\n",
            "Cost after iteration 200: 0.004552\n",
            "Cost after iteration 0: 0.007959\n",
            "Cost after iteration 400: 0.009484\n",
            "Cost after iteration 0: 0.007886\n",
            "Cost after iteration 100: 0.002229\n",
            "Cost after iteration 200: 0.004478\n",
            "Cost after iteration 300: 0.005961\n",
            "Cost after iteration 100: 0.002196\n",
            "Cost after iteration 200: 0.004407\n",
            "Cost after iteration 0: 0.007655\n",
            "Cost after iteration 300: 0.005980\n",
            "Cost after iteration 400: 0.009371\n",
            "Cost after iteration 200: 0.004413\n",
            "Cost after iteration 100: 0.002105\n",
            "Cost after iteration 0: 0.007660\n",
            "Cost after iteration 400: 0.009204\n",
            "Cost after iteration 300: 0.005852\n",
            "Cost after iteration 0: 0.007500\n",
            "Cost after iteration 100: 0.002086\n",
            "Cost after iteration 400: 0.009092\n",
            "Cost after iteration 200: 0.004329\n",
            "Cost after iteration 300: 0.005794\n",
            "Cost after iteration 300: 0.005733\n",
            "Cost after iteration 400: 0.008839\n",
            "Cost after iteration 200: 0.004279\n",
            "Cost after iteration 100: 0.002054\n",
            "Cost after iteration 0: 0.007360\n",
            "Cost after iteration 100: 0.002036\n",
            "Cost after iteration 0: 0.007288\n",
            "Cost after iteration 200: 0.004165\n",
            "Cost after iteration 300: 0.005543\n",
            "Cost after iteration 400: 0.008816\n",
            "Cost after iteration 300: 0.005612\n",
            "Cost after iteration 0: 0.007161\n",
            "Cost after iteration 100: 0.002028\n",
            "Cost after iteration 200: 0.004149\n",
            "Cost after iteration 400: 0.008766\n",
            "Cost after iteration 200: 0.004108\n",
            "Cost after iteration 100: 0.002009\n",
            "Cost after iteration 300: 0.005522\n",
            "Cost after iteration 400: 0.008660\n",
            "Cost after iteration 0: 0.007042\n",
            "Cost after iteration 400: 0.008451\n",
            "Cost after iteration 200: 0.004004\n",
            "Cost after iteration 0: 0.006913\n",
            "Cost after iteration 300: 0.005405\n",
            "Cost after iteration 100: 0.001922\n",
            "Cost after iteration 0: 0.006850\n",
            "Cost after iteration 200: 0.003947\n",
            "Cost after iteration 400: 0.008349\n",
            "Cost after iteration 300: 0.005290\n",
            "Cost after iteration 100: 0.001879\n",
            "Cost after iteration 400: 0.008480\n",
            "Cost after iteration 200: 0.003913\n",
            "Cost after iteration 100: 0.001879\n",
            "Cost after iteration 0: 0.006758\n",
            "Cost after iteration 300: 0.005250\n",
            "Cost after iteration 0: 0.006702\n",
            "Cost after iteration 300: 0.005223\n",
            "Cost after iteration 100: 0.001828\n",
            "Cost after iteration 400: 0.008144\n",
            "Cost after iteration 200: 0.003888\n",
            "Cost after iteration 200: 0.003825\n",
            "Cost after iteration 0: 0.006573\n",
            "Cost after iteration 100: 0.001803\n",
            "Cost after iteration 400: 0.008035\n",
            "Cost after iteration 300: 0.005154\n",
            "Cost after iteration 200: 0.003800\n",
            "Cost after iteration 400: 0.007928\n",
            "Cost after iteration 0: 0.006486\n",
            "Cost after iteration 100: 0.001777\n",
            "Cost after iteration 300: 0.005080\n",
            "Cost after iteration 200: 0.003743\n",
            "Cost after iteration 100: 0.001793\n",
            "Cost after iteration 0: 0.006427\n",
            "Cost after iteration 400: 0.007859\n",
            "Cost after iteration 300: 0.004996\n",
            "Cost after iteration 300: 0.005005\n",
            "Cost after iteration 100: 0.001772\n",
            "Cost after iteration 0: 0.006361\n",
            "Cost after iteration 400: 0.007880\n",
            "Cost after iteration 200: 0.003676\n",
            "Cost after iteration 400: 0.007632\n",
            "Cost after iteration 200: 0.003647\n",
            "Cost after iteration 300: 0.004957\n",
            "Cost after iteration 0: 0.006284\n",
            "Cost after iteration 100: 0.001714\n",
            "Cost after iteration 100: 0.001677\n",
            "Cost after iteration 200: 0.003599\n",
            "Cost after iteration 0: 0.006184\n",
            "Cost after iteration 400: 0.007526\n",
            "Cost after iteration 300: 0.004854\n",
            "Cost after iteration 200: 0.003541\n",
            "Cost after iteration 400: 0.007497\n",
            "Cost after iteration 100: 0.001681\n",
            "Cost after iteration 0: 0.006114\n",
            "Cost after iteration 300: 0.004788\n",
            "Cost after iteration 100: 0.001650\n",
            "Cost after iteration 200: 0.003513\n",
            "Cost after iteration 400: 0.007370\n",
            "Cost after iteration 300: 0.004717\n",
            "Cost after iteration 0: 0.006003\n",
            "Cost after iteration 300: 0.004687\n",
            "Cost after iteration 400: 0.007326\n",
            "Cost after iteration 100: 0.001615\n",
            "Cost after iteration 200: 0.003454\n",
            "Cost after iteration 0: 0.005947\n",
            "Cost after iteration 300: 0.004645\n",
            "Cost after iteration 400: 0.007179\n",
            "Cost after iteration 100: 0.001593\n",
            "Cost after iteration 200: 0.003447\n",
            "Cost after iteration 0: 0.005873\n",
            "Cost after iteration 200: 0.003400\n",
            "Cost after iteration 100: 0.001590\n",
            "Cost after iteration 400: 0.007121\n",
            "Cost after iteration 0: 0.005804\n",
            "Cost after iteration 300: 0.004540\n",
            "Cost after iteration 200: 0.003369\n",
            "Cost after iteration 300: 0.004565\n",
            "Cost after iteration 100: 0.001565\n",
            "Cost after iteration 0: 0.005779\n",
            "Cost after iteration 400: 0.007132\n",
            "Cost after iteration 200: 0.003332\n",
            "Cost after iteration 100: 0.001527\n",
            "Cost after iteration 0: 0.005692\n",
            "Cost after iteration 400: 0.006903\n",
            "Cost after iteration 300: 0.004433\n",
            "Cost after iteration 300: 0.004434\n",
            "Cost after iteration 200: 0.003280\n",
            "Cost after iteration 0: 0.005642\n",
            "Cost after iteration 100: 0.001535\n",
            "Cost after iteration 400: 0.006883\n",
            "Cost after iteration 400: 0.006754\n",
            "Cost after iteration 0: 0.005567\n",
            "Cost after iteration 300: 0.004356\n",
            "Cost after iteration 100: 0.001509\n",
            "Cost after iteration 200: 0.003245\n",
            "Cost after iteration 0: 0.005497\n",
            "Cost after iteration 200: 0.003211\n",
            "Cost after iteration 100: 0.001489\n",
            "Cost after iteration 400: 0.006726\n",
            "Cost after iteration 300: 0.004331\n",
            "Cost after iteration 100: 0.001484\n",
            "Cost after iteration 200: 0.003192\n",
            "Cost after iteration 300: 0.004360\n",
            "Cost after iteration 400: 0.006696\n",
            "Cost after iteration 0: 0.005421\n",
            "Cost after iteration 100: 0.001463\n",
            "Cost after iteration 300: 0.004237\n",
            "Cost after iteration 0: 0.005378\n",
            "Cost after iteration 200: 0.003157\n",
            "Cost after iteration 400: 0.006601\n",
            "Cost after iteration 300: 0.004235\n",
            "Cost after iteration 100: 0.001437\n",
            "Cost after iteration 400: 0.006584\n",
            "Cost after iteration 200: 0.003134\n",
            "Cost after iteration 0: 0.005328\n",
            "Cost after iteration 400: 0.006430\n",
            "Cost after iteration 300: 0.004189\n",
            "Cost after iteration 0: 0.005237\n",
            "Cost after iteration 200: 0.003100\n",
            "Cost after iteration 100: 0.001424\n",
            "Cost after iteration 0: 0.005207\n",
            "Cost after iteration 200: 0.003065\n",
            "Cost after iteration 400: 0.006304\n",
            "Cost after iteration 300: 0.004031\n",
            "Cost after iteration 100: 0.001403\n",
            "Cost after iteration 0: 0.005156\n",
            "Cost after iteration 100: 0.001398\n",
            "Cost after iteration 300: 0.004086\n",
            "Cost after iteration 200: 0.003038\n",
            "Cost after iteration 400: 0.006389\n",
            "Cost after iteration 200: 0.003002\n",
            "Cost after iteration 400: 0.006226\n",
            "Cost after iteration 300: 0.004056\n",
            "Cost after iteration 0: 0.005081\n",
            "Cost after iteration 100: 0.001372\n",
            "Cost after iteration 0: 0.005052\n",
            "Cost after iteration 400: 0.006199\n",
            "Cost after iteration 200: 0.002969\n",
            "Cost after iteration 100: 0.001358\n",
            "Cost after iteration 300: 0.004020\n",
            "train accuracy: 100.0 %\n",
            "test accuracy: 86.66666666666667 %\n"
          ]
        }
      ],
      "source": [
        "d = model(xtrain_tfidf, train_y, xtest_tfidf, test_y, num_iterations = 100, learning_rate = .5, print_cost = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "hbPe9OLsWAgI"
      },
      "source": [
        "**Comment**: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test error is around 84%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. You'll build an even better classifier next week!\n",
        "\n",
        "Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the `index` variable) you can look at predictions on pictures of the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UINqBEZZWAgJ"
      },
      "source": [
        "Let's also plot the cost function and the gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBqvlvXfWAgJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "a21570cd-aa7f-48f8-e9b6-2f2012e0374f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot learning curve (with costs)\n",
        "costs = np.squeeze(d['costs'])\n",
        "plt.plot(costs)\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "R3uJmPfFWAgJ"
      },
      "source": [
        "**Interpretation**:\n",
        "You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9agHsQxWAgJ"
      },
      "source": [
        "## 6 - Further analysis (optional/ungraded exercise) ##\n",
        "\n",
        "Congratulations on building your first text classification model. Let's analyze it further, and examine possible choices for the learning rate $\\alpha$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYQpb_lhWAgJ"
      },
      "source": [
        "#### Choice of learning rate ####\n",
        "\n",
        "**Reminder**:\n",
        "In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate $\\alpha$  determines how rapidly we update the parameters. If the learning rate is too large we may \"overshoot\" the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That's why it is crucial to use a well-tuned learning rate.\n",
        "\n",
        "Let's compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the `learning_rates` variable to contain, and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2v_2D5VgWAgJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "outputId": "281a4f74-ba9a-4d3d-bbf4-59c44d2eb507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning rate is: 0.05\n",
            "train accuracy: 100.0 %\n",
            "test accuracy: 86.0 %\n",
            "\n",
            "-------------------------------------------------------\n",
            "\n",
            "learning rate is: 0.1\n",
            "train accuracy: 100.0 %\n",
            "test accuracy: 86.0 %\n",
            "\n",
            "-------------------------------------------------------\n",
            "\n",
            "learning rate is: 0.5\n",
            "train accuracy: 100.0 %\n",
            "test accuracy: 85.33333333333334 %\n",
            "\n",
            "-------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "learning_rates = [0.05, 0.1, 0.5]\n",
        "models = {}\n",
        "for i in learning_rates:\n",
        "    print (\"learning rate is: \" + str(i))\n",
        "    models[str(i)] = model(xtrain_tfidf, train_y, xtest_tfidf, test_y, num_iterations = 3000, learning_rate = i, print_cost = False)\n",
        "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
        "\n",
        "for i in learning_rates:\n",
        "    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n",
        "\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (hundreds)')\n",
        "\n",
        "legend = plt.legend(loc='upper center', shadow=True)\n",
        "frame = legend.get_frame()\n",
        "frame.set_facecolor('0.90')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhSWhAK8WAgJ"
      },
      "source": [
        "**Interpretation**:\n",
        "- Different learning rates give different costs and thus different predictions results.\n",
        "- If the learning rate is too large, the cost may oscillate up and down. It may even diverge (though in this example, using 0.5 does not have that problem and still eventually ends up at a good value for the cost).\n",
        "- A lower cost doesn't mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.\n",
        "- In deep learning, we usually recommend that you:\n",
        "    - Choose the learning rate that better minimizes the cost function.\n",
        "    - If your model overfits, use other techniques to reduce overfitting. (We'll talk about this in later videos.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "DyxFBSzZWAgJ"
      },
      "source": [
        "## 7 - Test with your own news (optional/ungraded exercise) ##\n",
        "\n",
        "Congratulations on finishing this assignment. You can use your own text and see the output of your model.\n",
        "\n",
        "[\"Chinhtrixahoi\" \"Thethao\" \"Khac\"] labels corresponds to [0 2 1] labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixUK4Eu2WAgJ"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"Làm mẹ khi quá trẻ dễ bị loãng xương\n",
        "Những cô gái mới lớn mang thai sẽ gia tăng nguy cơ xương bị yếu đi. Trong một nghiên cứu mới tại Mỹ, 1/3 số bà mẹ ở tuổi thiếu niên có chỉ số điển hình của bệnh loãng xương, hoặc có dấu hiệu báo trước căn bệnh này.\n",
        "\"Cần phải đảm bảo rằng những bà mẹ thiếu niên tiêu thụ đủ lượng canxi trong thời gian mang thai - 1.300 milligram mỗi ngày, để đáp ứng nhu cầu canxi của cả mẹ lẫn bào thai\", Kimberly O. O'Brien tại Trường sức khoẻ cộng đồng Johns Hopkins Bloomberg ở Baltimore, phát biểu.\n",
        "Canxi đặc biệt cần thiết trong thời kỳ mang thai do bào thai khi lớn lên cần nhiều chất dinh dưỡng để hình thành xương, trong lúc bản thân các thiếu nữ cũng cần nhiều canxi cho chính mình. Thực tế, 40% lượng xương của con gái được hình thành trong độ tuổi dậy thì.\n",
        "Mặc dù có hơn nửa triệu thiếu nữ sinh con tại Mỹ mỗi năm, chưa có nhiều thông tin về việc mang thai ảnh hưởng thế nào tới xương của người mẹ. O'Brien và cộng sự đã nghiên cứu 23 cô gái mang thai trong độ tuổi 13,5 đến 18,3. Cũng giống như người lớn, lượng tiêu thụ canxi trong thời kỳ mang thai của các cô gái trẻ cao hơn là sau khi sinh.\n",
        "Khoảng 1/3 các bà mẹ trẻ có dấu hiệu xương mỏng đi đáng kể sau khi sinh. Trong số 15 em được đo xương trong 3-4 tháng sau khi sinh, 2 em có đủ dấu hiệu của bệnh loãng xương. 3 em khác có dấu hiệu của tiền loãng xương.\n",
        "Tuy vậy, không phải cứ mang thai ở độ tuổi thiếu niên là có xương bị yếu đi. Kết quả nghiên cứu chỉ ra rằng tiêu thụ nhiều lượng canxi hơn trong thời kỳ mang thai sẽ giúp chống lại tình trạng yếu xương.\n",
        "\"\"\"\n",
        "\n",
        "x_tfidf =  tfidf_vect.transform([text]).T.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2eW_flmWAgK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "378ce68a-d31d-4198-8e12-6ad0d9ee237f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1]),\n",
              " array([[0.13478959],\n",
              "        [0.81421456],\n",
              "        [0.05099585]]))"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ],
      "source": [
        "predict(d[\"w\"], d[\"b\"], x_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Liverpool vừa vào chung kết Cup Liên đoàn Anh gặp Chelsea, và đang giữ đỉnh bảng Ngoại hạng Anh, vẫn còn khả năng ăn bốn danh hiệu. Vì thế, quyết định của ông gây ngỡ ngàng. Nhưng HLV người Đức cho biết ông đã nói chuyện với giới chủ Liverpool từ tháng 11/2023. Và quyết định này không liên quan gì đến sức khỏe của ông.\n",
        "\"\"\"\n",
        "\n",
        "x_tfidf =  tfidf_vect.transform([text]).T.toarray()"
      ],
      "metadata": {
        "id": "XeBsUu7m48S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(d[\"w\"], d[\"b\"], x_tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsjgUif75G7A",
        "outputId": "51439949-128e-4cbb-abe9-da36bd7d868a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2]),\n",
              " array([[0.00409019],\n",
              "        [0.20098161],\n",
              "        [0.7949282 ]]))"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN0rIQvSWAgK"
      },
      "source": [
        "<font color='blue'>\n",
        "**What to remember from this assignment:**\n",
        "1. Preprocessing the dataset is important.\n",
        "2. You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model().\n",
        "3. Tuning the learning rate (which is an example of a \"hyperparameter\") can make a big difference to the algorithm. You will see more examples of this later in this course!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVsIQ4O2WAgK"
      },
      "source": [
        "Finally, if you'd like, we invite you to try different things on this Notebook. Make sure you submit before trying anything. Once you submit, things you can play with include:\n",
        "    - Play with the learning rate and the number of iterations\n",
        "    - Try different initialization methods and compare the results\n",
        "    - Test other preprocessings\n",
        "    - Try others parameters of TfidfVectorizer, such as ngrams, lowercase\n",
        "    - See what features are in a Tf-idf vector\n",
        "    - Test with different feature representation other than Tf-idf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8N0ORpMWAgK"
      },
      "source": [
        "Bibliography:\n",
        "- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n",
        "- https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
      ]
    }
  ],
  "metadata": {
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}